import ast
import os
import math


class Behave():

	def __init__(self, *args, **kwargs):

		self.data = []

		if (kwargs.get("file")):

			file_name = str(kwargs.get("file"))

			self.data = self.read_from_file(self.PATH+file_name)


		if (kwargs.get("data")):

			self.data = kwargs.get("data")


		# if the passed in data is text, convert it to json
		if (type(self.data) == str):

			self.data = ast.literal_eval(self.data)


	def read_from_file(self, path):

		f = open(path, "r")

		file_contents = f.read()

		return file_contents


	@staticmethod
	def average(arr):
		"""calculate average from a given array of data
		
		Args:
			arr (Array): Input array
		
		Returns:
			Array: Calculated average
		"""

		return sum(arr)/len(arr)

	@staticmethod
	def standart_deviation(arr):
		"""Calculate the standart deviation of the array
		
		Args:
			arr (Array): Input array
		
		Returns:
			Array: Calculated deviation
		"""

		length = len(arr)

		average = sum(arr)/length

		deviations = [(i-average)**2 for i in arr]

		variance = sum(deviations)/length

		deviation = math.sqrt(variance)

		return deviation


	def normalize_data(self, min_max_values):

		user_data = self.get_all_params()

		user_data_values = user_data.values()

		normalized_data = []

		for key in range(len(user_data_values)):

			minv = min_max_values[key][0]
			maxv = min_max_values[key][1]

			value = user_data_values[key]

			calc_val = (value-minv)/(maxv-minv)

			normalized_data.append(calc_val)

		return tuple(normalized_data)

	@staticmethod
	def normalize_training_data(training_data):
		"""Normalizes the passed training data and prepares it for use in neural network.
		
		Args:
			training_data (array): The array should consist of dictionaries. Each should have two indexes: 
				1) 'data' - the data generated by the get_all_params() method
				2) 'response' - index of the neuron which must be set to 1.

		Returns:
			dict: Returns a dictionary with the following indexes:
				1) 'min_max': minimum and maximum values used in normalization for this chunk of data
				2) 'data' : tuples with input data for training
				3) 'responses' : tuples with responses for training
		"""

		params_arr = []
		resp = []
		resp_ids = []
		min_max_values = []
		avg_data_arr = []


		for i in range(len(training_data)):

			params_dict = training_data[i]['data']

			params_arr.append([params_dict[key]*1.0 for key in params_dict])

			resp_ids.append(training_data[i]['response'])


		for i in range(len(params_dict)):

			param_data = zip(*params_arr)[i]

			maxv = max(param_data)
			minv = min(param_data)

			min_max = (minv, maxv)

			min_max_values.append(min_max)

			avg_data = [(val-minv)/(maxv-minv) for val in param_data]

			avg_data_arr.append(avg_data)


		data = [zip(*avg_data_arr)[i] for i in range(len(training_data))]

		maxlen = max(resp_ids)+1

		for i in range(len(resp_ids)):
			new_arr = []
			for j in range(maxlen):

				if j==resp_ids[i]:
					new_arr.append(1)
					continue

				new_arr.append(0)

			arr_tuple = tuple(new_arr)
			resp.append(arr_tuple)


		user_data = {
			"data" : data,
			"responses" : resp,
			"min_max" : min_max_values
		}

		return user_data